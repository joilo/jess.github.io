# Markov Chains

标签（空格分隔）： stat.

---

## 4.1 Introduction
We suppose that whenever the process is in state i, there is a fixed probability $P_{i,j}$ that it will next be in state j. That is, we suppose that 
$$P(X_{n+1}=j|X_{n}=i,X_{n-1}=i_{n-1}...,X_{1}=i_{1},X_{0}=i_{0})=P_{i,j}$$

* Random Walk Model
$$P_{i,i+1}=p=1-P_{i,i-1}$$
* Gambling Model
$$P_{i,i+1}=p=1-P_{i,i-1}$$ $$P_{00}=P_{NN}=1$$
States $0$ and $N$ are called *absorbing* states.
Finite state random walk with absorbing barriers.

## 4.2 Chapman-Kolmogorov Equations

* The Chapman-Kolmogorov equations provide a method for computing these n-step transition probabilities.
$$P_{ij}^{n+m}=\sum_{k=0}^\infty P_{ik}^nP_{kj}^m$$ 
for all $n, m \geq 0$, all $i,j$
$$\mathbf{P}^{(n+m)}=\mathbf{P}^{(n)}\cdot \mathbf{P}^{(m)}$$

* Transform all states in $\mathscr{A}$ into absorbing states
$$P\{X_{m+1}=j|X_{m}=i\}=
\begin{cases}
1, & \text{if } i\in \mathscr{A},\ j=i\\
0, & \text{if } i\in \mathscr{A},\ j \ne i
\end{cases}$$

## 4.3 Classification of States
* State $j$ is said to be *accessible* from state $i$ if $P^n_{ij}>0$ for some $N\geq 0$.

* Two states $i$ and $j$ are accessible to each other are said to **communicate**, and we write $i\leftrightarrow j$
 
    * Three properties:
 (i) State $i$ communicates with state $i$, all $i\geq 0$.
 (ii) If state $i$ communicates with state $j$, then state $j$ communicates with state $i$.
 (iii) If state $i$ communicates with state $j$, and state $j$ communicates with state $k$, then state $i$ communicates with state $k$.

* Two states that communicate are said to be in the same **class**.
* The Markov chain is said to be **irreducible** if there is only one class, that is, if all states communicate with each other. 

* $f_i=P(starting in state i, the process will ever reenter state i)$
 **recurrent**: $f_i=1$
 **transient**: $f_i<1$

    Easy to know that, state i is recurrent if and only if, starting in state i, the expected number of timeperiods that the process is in state i is infinite. But, letting
    $$I_n=
    \begin{cases}
    1, & \text{if } X_n=i \\
    0, & \text{if } X_n\ne i
    \end{cases}$$
 we have that $\sum_{n=0}^\infty I_n$ represents the number of periods that the process is in state i. Also,
 \begin{aligned}
 E[\sum_{n=0}^\infty I_n|X_0=i]&=\sum_{n=0}^\infty E[I_n|X_0=i]\\
 & =\sum_{n=0}^\infty P\{X_n=i|X_0=i\}\\
 & =\sum_{n=0}^\infty P_{ii}^n
 \end{aligned}
 We have thus proven the following.
 
 **Proposition 4.1** State i is 
recurrent if $\sum_{n=1}^\infty P_{ii}^n=\infty$
transient if $\sum_{n=1}^\infty P_{ii}^n<\infty$ 
>  $?=1/(1-f_i)$

    - In a finite-state Markov chain not all states can be transient.
    
 **Corollary 4.2** If state i is recurrent, and state i communicate with state j, then state j is reccurent.

    - transient is a class property
    - all states of a finite irreducible Markov chain are recurrent
    
 **Example 4.15** (Random Walk)
    - $p=\frac{1}{2}$ recurrent, $\quad p\ne \frac{1}{2}$ transient
    - Symmetric randm walks in 1 and 2 dimensions are both recurrent, all higher-dimension symmetric random walks turn out to be transient.     
    - nonsymmetric case: $P_{ever return to 0}=2min(p,1-p)$
    
## 4.4. Long-Run Proportions and Limiting Probabilities

- $$f_{i,j}=P(X_n=j\ for\ some\ n>0|X_0=i)$$

 **Proporsition 4.3**
 If $i$ is recurrent and $i$ communicates with $j$, then $f_{i,j} =1$
 **Proof.**
 Because the number of opportunities until the ﬁrst success occurs is geometric with parameter $P^n_{i,j}$, it follows that *with probability 1* a success will eventually occur and so, with probability 1, state j will eventually be entered. 
>- Converges almost surely or almost everywhere or *with probability 1* or strongly towards **X** means that 
$$Pr(\lim_{n \rightarrow 0}X_n=X)=1$$

- If state j is recurrent, let $m_j$ denote the expected number of transitions that it takes the Markov chain when starting in state j to return to that state. That is, with 
$$N_j =min\{n > 0: X_n = j\} $$

 equal to the number of transitions until the Markov chain makes a transition into state j, 
$$m_j = E[N_j|X_0 = j]$$

 **Deﬁnition**:  Say that the recurrent state j is **positive recurrent** if $m_j < ∞$ and say that it is **null recurrent** if $m_j =∞$ 

 letting $π_j$ denote the long-run proportion of time that the Markov chain is in state j, we have the following proposition. 
 **Proposition 4.4** 
 If the Markov chain is irreducible and recurrent, then for any initial state 
 $$π_j =1/m_j$$ 
 
    $$\pi_j=\lim_{n \rightarrow \infty}\frac{n}{\sum_{i=1}^{n}T_i}
= \lim_{n→∞}\frac{1}{\frac{1}{n}\sum_{i=1}^{n}T_i}
= \lim_{n→∞}\frac{1}{\frac{T_1}{n}+\frac{T_2+...+T_n}{n}}
=\frac{1}{m_j}
$$

 **Proposition 4.5** If i is positive recurrent and i ↔ j then j is positive recurrent. 
    - It follows from the preceding result that null recurrence is also a class property.
    - An irreducible finite state Markov chain must be positive recurrent. 
    
 **Theorem 4.1** Consider an irreducible Markov chain. If the chain is positive recurrent then the long-run proportions are the unique solution of the equations.
 $$\pi_j=\sum_i \pi_i p_{ij}, \  j\geq 1$$
 
 $$\sum_i \pi_i=1$$
 
    >How to prove you have the solution? How to theoretically prove you don't     have more than one solution?
    $\pi=\pi A, A^T\pi^T=\pi^T(*)$ which means $A^T$ has eigenvalue 1 and     eigenvector $\pi^T$
    Theoretcally, prove irreducible and positve recurrent markov cain has unique solution of $\pi$ to the equation(*)
 
    - Moreover, if there is no solution of the preceding linear equations, then the Markov chain is either transient or null recurrent and all $π_j =0$.
    
- The long run proportions $π_j, j \geq 0$, are often called **stationary probabilities**. The reason being that if the initial state is chosen according to the probabilities $π_j, j \geq 0$, then the probability of being in state j at any time n is also equal to $π_j$. That is, if 
$$P_{X_0 = j}=π_j, j \geq 0$$

 then
$$P\{X_n = j\}=π_j \quad for\  all\  n, j \geq 0$$ 
 
- Let $μ(i,i_1)$ be the mean number of transitions for the chain to enter state $i_1$, given that the initial state is $i$, $i \geq 0$.The quantities $μ(i,i_1)$ can be determined as the solution of the following set of equations, obtained by conditioning on the first transition out of state $i$:
 $$μ(i,i_1) =1+\sum_{j\ne i_1}P_{i,j}u_{j,i_1},\quad i\geq 0$$

- **Proposition 4.6** Let $\{X_n, n \geq 1\}$ be an irreducible Markov chain with stationary probabilities $π_j$, $j\geq 0$, and let $r$ be a bounded function on the state space. Then, with probability 1,
 $$\lim_{N\rightarrow \infty} \frac{\sum_{n=1}^{N}r(X_n)}{N}=\sum_{j=0}^{\infty}r(j)\pi_j$$

 If we suppose that we earn a reward $r(j)$ whenever the chain is in state j, then *Proposition 4.6* states that our average reward per unit time is $\sum_j r(j)π_j$
 
- It is not always true that the long-run proportions are also limiting probabilities. To see why not, consider a two-state Markov chain having 
 $$P_{0,1} = P_{1,0} =1 $$

 the long-run proportions of time it spends in these states are 
 $$π0 = π_1 =1/2 $$

 However, $P^n_{0,0}$ does not have a limiting value as n goes to inﬁnity.
 
  - In general, a chain that can only return to a state in a multiple of $d > 1$ steps (where $d = 2$ in the preceding example) is said to be **periodic** and does **not have limiting probabilities**.  
  - However, for an **irreducible** chain that is not periodic, and such chains are called **aperiodic**, the limiting probabilities will **always exist** and will **not depend on the initial state**. Moreover, the limiting probability that the chain will be in state $j$ will equal $π_j$, the long-run proportion of time the chain is in state j.
    **Proof** (*the limiting probabilities,when they exist, will equal the long-run proportions*)
$$α_j = \lim_{n\rightarrow \infty}P(X_n=j)$$

    $$\begin{cases}
P(X_{n+1} = j) =\sum_{i=0}^\infty P_{ij}P(X_n=i)\\
1=\sum_{i=0}^∞ P(X_n =i) 
\end{cases}$$

    Letting $n → ∞$
    $$\begin{cases}
α_j =\sum_{i=0}^\infty α_i P_{ij}\\
1=\sum_{i=0}^\infty α_i
\end{cases}$$
   
     Hence, $\{α_j, j \geq 0\}$ satisﬁes the equations for which $\{π_j, j \geq 0\}$ is the unique solution, showing that $α_j = π_j, j \geq 0$.
     
* The Gambler's Ruin Problem
 $$P_i=
\begin{cases}
\frac{1-(\frac{q}{p})^i}{1-(\frac{q}{p})^N}, &\text{if } p\neq \frac{1}{2}\\
\frac{i}{N},&\text{if }p=\frac{1}{2}
\end{cases}
$$

 Note that as $N\rightarrow \infty$, 
 $$P_i=
\begin{cases}
1-(\frac{q}{p})^i, &\text{if } p>\frac{1}{2}\\
0,&\text{if }p\leq\frac{1}{2}
\end{cases}
$$







    
    
